{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\soft\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from conv_rmm_cell import ConvCell\n",
    "from data import get_features\n",
    "from utils import plot_confusion_matrix, cross_entropy, accuracy, get_spectogram\n",
    "\n",
    "import librosa\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HARM = 14\n",
    "N_CAND = 100\n",
    "SEQ_LEN = 16\n",
    "N_HIDDEM = 16\n",
    "BATCH_SIZE = 64\n",
    "# N_CLASS = 3\n",
    "\n",
    "START_LR = 0.05\n",
    "TRAINING_STEPS = 500000\n",
    "LR_PERIONS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    filenames = []\n",
    "    for f in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, f)) and f.endswith('.npz'):\n",
    "            data = np.load(os.path.join(path, f))\n",
    "            features = data['features']\n",
    "            if features.shape[0] > SEQ_LEN:\n",
    "                filenames.append(os.path.join(path, f))\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "# DATA_PATH = 'C:/Dataset/noisy_speech'\n",
    "DATA_PATH = 'D:/musan/preprocessed/noisy_speech_v2'\n",
    "FOLDERS = ['train', 'valid']\n",
    "\n",
    "train_files = np.array([(filename, True) for filename in get_files(os.path.join(DATA_PATH, FOLDERS[0]))])\n",
    "np.random.shuffle(train_files)\n",
    "\n",
    "valid_files = np.array([(filename, False) for filename in get_files(os.path.join(DATA_PATH, FOLDERS[1]))])\n",
    "np.random.shuffle(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std = np.load('mean-std.npz')\n",
    "\n",
    "mean = mean_std['mean'][np.newaxis, np.newaxis, ...]\n",
    "std = mean_std['std'][np.newaxis, np.newaxis, ...]\n",
    "\n",
    "mean = tf.constant(mean, name='mean')\n",
    "std = tf.constant(std, name='std')\n",
    "\n",
    "seq_len_t = tf.constant(SEQ_LEN, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    path = path.decode()\n",
    "    data = np.load(path)\n",
    "    features = data['features']\n",
    "    labels = data['labels'].astype(np.float32)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(features, labels):\n",
    "    stride = SEQ_LEN // 2\n",
    "    n_samples = features.shape[0] // stride  - SEQ_LEN + 1\n",
    "    if n_samples > 0:\n",
    "        batch_features = np.zeros(shape=(n_samples, SEQ_LEN, N_CAND, N_HARM), dtype=np.float32)\n",
    "        batch_targets = np.zeros(shape=(n_samples, SEQ_LEN, 1), dtype=np.float32)\n",
    "\n",
    "        labels = labels[..., np.newaxis]\n",
    "        for i in range(n_samples):\n",
    "            batch_features[i] = features[i * stride:i * stride+SEQ_LEN]\n",
    "            batch_targets[i] = labels[i * stride:i * stride+SEQ_LEN]\n",
    "\n",
    "        return batch_features, batch_targets\n",
    "    else:\n",
    "        batch_features = np.zeros(shape=(1, SEQ_LEN, N_CAND, N_HARM), dtype=np.float32)\n",
    "        batch_targets = np.zeros(shape=(1, SEQ_LEN, 1), dtype=np.float32)\n",
    "        \n",
    "        _features = features[:SEQ_LEN]\n",
    "        _labels = labels[:SEQ_LEN]\n",
    "        batch_features[0, :_features.shape[0]] = _features\n",
    "        batch_targets[:, :_features.shape[0], 0] = _labels\n",
    "        \n",
    "        return batch_features, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('dataset'):\n",
    "    filenames = tf.placeholder(dtype=train_files.dtype, name='filename')\n",
    "#     is_speech_phr = tf.placeholder(dtype=tf.bool, name='is_speech')\n",
    "\n",
    "    dataset = tf.data.Dataset().from_tensor_slices((filenames))\n",
    "    dataset = dataset.map(lambda filename: tf.py_func(load_file, [filename], [tf.float32, tf.float32]))\n",
    "    dataset = dataset.cache('C:/Dataset/cache/')\n",
    "    dataset = dataset.map(lambda features, labels: tf.py_func(prepare_data, [features, labels], [tf.float32, tf.float32]), 4)\n",
    "#     dataset = dataset.map(lambda features, labels, length: prepare_data(features, labels, length), 4)\n",
    "    dataset = dataset.flat_map(lambda *samples: tf.data.Dataset().from_tensor_slices(samples))\n",
    "    dataset = dataset.map(lambda features, labels: (tf.reshape(features, [SEQ_LEN, N_CAND, N_HARM]), tf.reshape(labels, [SEQ_LEN, 1])))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(20000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "    \n",
    "    train_iterator = dataset.make_initializable_iterator()\n",
    "    valid_iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    train_batch = train_iterator.get_next()\n",
    "    valid_batch = valid_iterator.get_next()\n",
    "    \n",
    "    is_training = tf.placeholder_with_default(True, shape=None, name='is_training')\n",
    "\n",
    "    batch_features, batch_targets = tf.cond(is_training, \n",
    "                                      true_fn=lambda: train_batch, \n",
    "                                      false_fn=lambda: valid_batch)   \n",
    "    batch_features = tf.reshape(batch_features, [-1, SEQ_LEN, N_CAND, N_HARM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('standardization'):\n",
    "    nnet = batch_features - mean\n",
    "    nnet /= std\n",
    "\n",
    "with tf.variable_scope('CNN'):\n",
    "    nnet = tf.layers.conv2d(nnet, N_HIDDEM, kernel_size=[1, 1], activation=tf.nn.relu, name='Cond2D_1')\n",
    "    conv_output = tf.layers.conv2d(nnet, 1, kernel_size=[1, 1], activation=tf.nn.sigmoid, name='Cond2D_2')\n",
    "\n",
    "    \n",
    "with tf.name_scope('CNN_output'):\n",
    "    conv = tf.reshape(conv_output, [-1, N_CAND])\n",
    "    conv_predictions = tf.reduce_max(conv, axis=1)\n",
    "    \n",
    "with tf.variable_scope('RNN'):\n",
    "    cell = ConvCell((SEQ_LEN, N_CAND, 1), filters=1, kernel_size=32, padding='SAME', activation=tf.nn.sigmoid, name='recurent_cell')\n",
    "    rnn_output, state = tf.nn.dynamic_rnn(cell, conv_output, sequence_length=[SEQ_LEN] * BATCH_SIZE, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('RNN_output'):\n",
    "    nnet = tf.transpose(rnn_output, (1, 0, 2, 3))\n",
    "    nnet = tf.gather(nnet, int(nnet.get_shape()[0]) - 1, name=\"last_rnn_output\")\n",
    "#     nnet = tf.reshape(rnn_output, [-1, N_CAND])\n",
    "\n",
    "logits = tf.reduce_max(nnet, axis=(1, 2))\n",
    "# predictions = tf.sigmoid(logits)\n",
    "predictions = logits\n",
    "\n",
    "with tf.name_scope('postprocessing'):\n",
    "    classes = tf.cast(predictions > 0.5, tf.int32)\n",
    "    cnn_classes = tf.cast(conv_predictions > 0.5, tf.int32)\n",
    "    \n",
    "    cnn_targets = tf.reshape(batch_targets, [-1, 1])\n",
    "    cnn_targets = cnn_targets[:, 0]\n",
    "\n",
    "    targets = tf.transpose(batch_targets, [1, 0, 2])\n",
    "    targets = tf.gather(targets, int(targets.get_shape()[0]) - 1, name=\"last_batch_target\")[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_cost = cross_entropy(targets=cnn_targets, predictions=conv_predictions)\n",
    "cnn_loss = tf.reduce_mean(cnn_cost)\n",
    "\n",
    "output_cost = cross_entropy(targets=targets, predictions=predictions)\n",
    "output_loss = tf.reduce_mean(output_cost)\n",
    "\n",
    "loss = output_loss + cnn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\soft\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, dtype=tf.int32)\n",
    "lr_period_steps = int(TRAINING_STEPS / LR_PERIONS)\n",
    "global_step_update_op = tf.cond(global_step < tf.convert_to_tensor(lr_period_steps), \n",
    "                                true_fn=lambda: global_step.assign(global_step + 1),\n",
    "                               false_fn=lambda: global_step.assign(0))\n",
    "\n",
    "lr = tf.train.linear_cosine_decay(START_LR, global_step, lr_period_steps)\n",
    "\n",
    "\n",
    "cnn_trainable_variables = tf.trainable_variables('CNN')\n",
    "cnn_grads = tf.gradients(cnn_loss, cnn_trainable_variables, name='gradients_cnn')\n",
    "cnn_optimizer = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "cnn_train_op = cnn_optimizer.apply_gradients(zip(cnn_grads, cnn_trainable_variables))\n",
    "\n",
    "rnn_trainable_variables = tf.trainable_variables('RNN')\n",
    "rnn_grads = tf.gradients(output_loss, rnn_trainable_variables, name='gradients_rnn')\n",
    "rnn_optimizer = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "rnn_train_op = rnn_optimizer.apply_gradients(zip(rnn_grads, rnn_trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_accuracy = accuracy(cnn_classes, tf.cast(cnn_targets, tf.int32))\n",
    "rnn_accuracy = accuracy(classes, tf.cast(targets, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cm = tf.confusion_matrix(labels=tf.cast(targets, tf.int32), predictions=classes, num_classes=2)\n",
    "cnn_cm = tf.confusion_matrix(labels=tf.cast(cnn_targets, tf.int32), predictions=cnn_classes, num_classes=2)\n",
    "\n",
    "def _plot_confusion_matrix_wrapper(confusion_matrix):\n",
    "    return plot_confusion_matrix(confusion_matrix, {'Noise': 0, 'Voice': 1})\n",
    "\n",
    "rnn_confusion_matrix_img = tf.py_func(_plot_confusion_matrix_wrapper, [rnn_cm], tf.uint8)\n",
    "rnn_confusion_matrix_img = tf.expand_dims(rnn_confusion_matrix_img, axis=0)\n",
    "\n",
    "cnn_confusion_matrix_img = tf.py_func(_plot_confusion_matrix_wrapper, [cnn_cm], tf.uint8)\n",
    "cnn_confusion_matrix_img = tf.expand_dims(cnn_confusion_matrix_img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "\n",
    "for grad in cnn_grads + rnn_grads:\n",
    "    if not ('bias' in grad.name.lower()):\n",
    "        tf.summary.scalar('gradients/'+grad.name, tf.reduce_mean(tf.abs(grad)))\n",
    "        tf.summary.histogram('gradients/'+grad.name, grad)\n",
    "        \n",
    "for param in tf.trainable_variables():\n",
    "    if not ('bias' in param.name.lower()):\n",
    "        tf.summary.histogram('parameters/'+param.name, param)\n",
    "        \n",
    "tf.summary.scalar('learning_rate', lr)\n",
    "summaries.append(tf.summary.scalar('losses/rnn_loss', output_loss))\n",
    "# tf.summary.scalar('losses/l1_loss', l1_loss)\n",
    "summaries.append(tf.summary.scalar('losses/total_loss', loss))\n",
    "summaries.append(tf.summary.scalar('losses/cnn_loss', cnn_loss))\n",
    "summaries.append(tf.summary.scalar('accuracy/rnn', rnn_accuracy))\n",
    "summaries.append(tf.summary.scalar('accuracy/cnn', cnn_accuracy))\n",
    "\n",
    "# tf.summary.audio()\n",
    "\n",
    "summaries.append(tf.summary.image('confusion-matrix/rnn', rnn_confusion_matrix_img))\n",
    "summaries.append(tf.summary.image('confusion-matrix/cnn', cnn_confusion_matrix_img))\n",
    "\n",
    "\n",
    "train_summary_op = tf.summary.merge_all()\n",
    "valid_summary_op = tf.summary.merge(summaries)\n",
    "\n",
    "# spectr_summary_op = tf.summary.image('spectrogramm', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "checkpoint_path = os.path.join('trained_model/new', 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 499500. Training loss: 0.64482. Validation loss: 0.73382. Time: 42.70354175567627"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "options = tf.RunOptions(trace_level = tf.RunOptions.NO_TRACE)\n",
    "\n",
    "train_writer = tf.summary.FileWriter('logdir/new/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter('logdir/new/valid')\n",
    "\n",
    "sess.run(tf.global_variables_initializer(), options=options)\n",
    "\n",
    "sess.run(train_iterator.initializer, \n",
    "         feed_dict={filenames: train_files[:, 0]}, options=options)\n",
    "\n",
    "sess.run(valid_iterator.initializer, \n",
    "         feed_dict={filenames: valid_files[:, 0]}, options=options)\n",
    "sess.graph.finalize()\n",
    "\n",
    "started = False\n",
    "for i in range(TRAINING_STEPS):\n",
    "    if not started:\n",
    "        start = time.time()\n",
    "        started = True\n",
    "        \n",
    "    _ = sess.run(cnn_train_op, options=options)\n",
    "    _ = sess.run(rnn_train_op, options=options)\n",
    "    \n",
    "    sess.run(global_step_update_op)\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        training_loss, summary  = sess.run([loss, train_summary_op], options=options)\n",
    "        train_writer.add_summary(summary, i)\n",
    "\n",
    "#         validation_loss = sess.run(loss, feed_dict={is_training: False}, options=options)\n",
    "\n",
    "        validation_loss, summary = sess.run([loss, valid_summary_op], feed_dict={is_training: False}, options=options)\n",
    "        valid_writer.add_summary(summary, i)\n",
    "\n",
    "        saver.save(sess, checkpoint_path, i)\n",
    "        \n",
    "        finish = time.time()\n",
    "        started = False\n",
    "        print('\\rStep %i. Training loss: %.5f. Validation loss: %.5f. Time: %s' % (i, training_loss, validation_loss, str(finish - start)), end='')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained_model/new\\\\model.ckpt-499999'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, checkpoint_path, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
